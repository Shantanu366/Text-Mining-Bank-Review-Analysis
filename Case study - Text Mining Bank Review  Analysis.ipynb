{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('D:/BankReviews.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>BankName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>5</td>\n",
       "      <td>Great job, Wyndham Capital! Each person was pr...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>5</td>\n",
       "      <td>Matthew Richardson is professional and helpful...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>5</td>\n",
       "      <td>We had a past experience with Wyndham Mortgage...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>5</td>\n",
       "      <td>We have been dealing with Brad Thomka from the...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-27</td>\n",
       "      <td>5</td>\n",
       "      <td>I can't express how grateful I am for the supp...</td>\n",
       "      <td>Wyndham Capital Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2016-02-06</td>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\nI never write reviews but had to this time...</td>\n",
       "      <td>North American Savings Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2016-07-25</td>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\nIt all started when Bob G ran a credit che...</td>\n",
       "      <td>North American Savings Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2017-09-27</td>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\nWhat a horrible experience. We have excell...</td>\n",
       "      <td>North American Savings Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2017-12-24</td>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\nRep was extremely professional, friendly, ...</td>\n",
       "      <td>North American Savings Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>2017-03-19</td>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\nI was working with a loan consultant from ...</td>\n",
       "      <td>North American Savings Bank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Stars                                            Reviews  \\\n",
       "0   2017-04-10      5  Great job, Wyndham Capital! Each person was pr...   \n",
       "1   2017-02-10      5  Matthew Richardson is professional and helpful...   \n",
       "2   2017-08-21      5  We had a past experience with Wyndham Mortgage...   \n",
       "3   2017-12-17      5  We have been dealing with Brad Thomka from the...   \n",
       "4   2016-05-27      5  I can't express how grateful I am for the supp...   \n",
       "..         ...    ...                                                ...   \n",
       "500 2016-02-06      1  \\r\\nI never write reviews but had to this time...   \n",
       "501 2016-07-25      1  \\r\\nIt all started when Bob G ran a credit che...   \n",
       "502 2017-09-27      1  \\r\\nWhat a horrible experience. We have excell...   \n",
       "503 2017-12-24      1  \\r\\nRep was extremely professional, friendly, ...   \n",
       "504 2017-03-19      1  \\r\\nI was working with a loan consultant from ...   \n",
       "\n",
       "                        BankName  \n",
       "0       Wyndham Capital Mortgage  \n",
       "1       Wyndham Capital Mortgage  \n",
       "2       Wyndham Capital Mortgage  \n",
       "3       Wyndham Capital Mortgage  \n",
       "4       Wyndham Capital Mortgage  \n",
       "..                           ...  \n",
       "500  North American Savings Bank  \n",
       "501  North American Savings Bank  \n",
       "502  North American Savings Bank  \n",
       "503  North American Savings Bank  \n",
       "504  North American Savings Bank  \n",
       "\n",
       "[505 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['Reviews']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words in the corpus also remove punctuations\n",
    "from nltk import RegexpTokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\").tokenize\n",
    "all_words = []\n",
    "for review in corpus:\n",
    "    words = tokenizer(review)\n",
    "    all_words = all_words+words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Great',\n",
       " 'job',\n",
       " 'Wyndham',\n",
       " 'Capital',\n",
       " 'Each',\n",
       " 'person',\n",
       " 'professional',\n",
       " 'helped',\n",
       " 'us',\n",
       " 'move',\n",
       " 'refinance',\n",
       " 'process',\n",
       " 'smoothly',\n",
       " 'Thank',\n",
       " 'Matthew',\n",
       " 'Richardson',\n",
       " 'professional',\n",
       " 'helpful',\n",
       " 'He',\n",
       " 'helped',\n",
       " 'us',\n",
       " 'find',\n",
       " 'correct',\n",
       " 'product',\n",
       " 'mortgage',\n",
       " 'Thank',\n",
       " 'much',\n",
       " 'excellent',\n",
       " 'service',\n",
       " 'Matthew',\n",
       " 'We',\n",
       " 'past',\n",
       " 'experience',\n",
       " 'Wyndham',\n",
       " 'Mortgage',\n",
       " 'would',\n",
       " 'without',\n",
       " 'question',\n",
       " 'use',\n",
       " 'needed',\n",
       " 'Wyndham',\n",
       " 'went',\n",
       " 'beyond',\n",
       " 'extra',\n",
       " 'mile',\n",
       " 'right',\n",
       " 'wrong',\n",
       " 'encountered',\n",
       " 'servicer',\n",
       " 'dealing',\n",
       " 'previous',\n",
       " 'loan',\n",
       " 'pulled',\n",
       " 'together',\n",
       " 'found',\n",
       " 'viable',\n",
       " 'option',\n",
       " 'us',\n",
       " 'ultimately',\n",
       " 'saved',\n",
       " 'us',\n",
       " 'money',\n",
       " 'We',\n",
       " 'would',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'Brad',\n",
       " 'Thomka',\n",
       " 'Wyndham',\n",
       " 'Capital',\n",
       " 'Mortgage',\n",
       " 'team',\n",
       " 'mortgage',\n",
       " 'needs',\n",
       " 'Sincerest',\n",
       " 'thanks',\n",
       " 'Wyndham',\n",
       " 'Ed',\n",
       " 'Lind',\n",
       " 'We',\n",
       " 'dealing',\n",
       " 'Brad',\n",
       " 'Thomka',\n",
       " 'beginning',\n",
       " 'started',\n",
       " 'stressful',\n",
       " 'time',\n",
       " 'us',\n",
       " 'help',\n",
       " 'Brad',\n",
       " 'entire',\n",
       " 'Wyndham',\n",
       " 'Mortgage',\n",
       " 'team',\n",
       " 'turned',\n",
       " 'happy',\n",
       " 'ending',\n",
       " 'Brad',\n",
       " 'Wyndham',\n",
       " 'went',\n",
       " 'beyond',\n",
       " 'extra',\n",
       " 'mile',\n",
       " 'right',\n",
       " 'wrong',\n",
       " 'encountered',\n",
       " 'servicer',\n",
       " 'dealing',\n",
       " 'previous',\n",
       " 'loan',\n",
       " 'pulled',\n",
       " 'together',\n",
       " 'found',\n",
       " 'viable',\n",
       " 'option',\n",
       " 'us',\n",
       " 'ultimately',\n",
       " 'saved',\n",
       " 'us',\n",
       " 'money',\n",
       " 'We',\n",
       " 'indebted',\n",
       " 'would',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'Brad',\n",
       " 'Thomka',\n",
       " 'Wyndham',\n",
       " 'Capital',\n",
       " 'Mortgage',\n",
       " 'team',\n",
       " 'mortgage',\n",
       " 'needs',\n",
       " 'Sincerest',\n",
       " 'thanks',\n",
       " 'Wyndham',\n",
       " 'Ed',\n",
       " 'LindRead',\n",
       " 'Less',\n",
       " 'I',\n",
       " 'express',\n",
       " 'grateful',\n",
       " 'I',\n",
       " 'support',\n",
       " 'Zach',\n",
       " 'provided',\n",
       " 'family',\n",
       " 'home',\n",
       " 'purchase',\n",
       " 'His',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'responsiveness',\n",
       " 'professional',\n",
       " 'demeanor',\n",
       " 'second',\n",
       " 'none',\n",
       " 'He',\n",
       " 'thorough',\n",
       " 'took',\n",
       " 'time',\n",
       " 'educate',\n",
       " 'process',\n",
       " 'along',\n",
       " 'way',\n",
       " 'I',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'working',\n",
       " 'Zach',\n",
       " 'Wyndham',\n",
       " 'Capital',\n",
       " 'I',\n",
       " 'pleasure',\n",
       " 'working',\n",
       " 'Wyndham',\n",
       " 'Capital',\n",
       " 'September',\n",
       " '2018',\n",
       " 'family',\n",
       " 'I',\n",
       " 'making',\n",
       " 'home',\n",
       " 'purchase',\n",
       " 'Columbus',\n",
       " 'OH',\n",
       " 'From',\n",
       " 'original',\n",
       " 'conversation',\n",
       " 'loan',\n",
       " 'officer',\n",
       " 'underwriting',\n",
       " 'way',\n",
       " 'closing',\n",
       " 'entire',\n",
       " 'team',\n",
       " 'Wyndham',\n",
       " 'responsive',\n",
       " 'professional',\n",
       " 'I',\n",
       " 'VERY',\n",
       " 'impressed',\n",
       " 'smooth',\n",
       " 'process',\n",
       " 'timely',\n",
       " 'communication',\n",
       " 'given',\n",
       " 'stressful',\n",
       " 'process',\n",
       " 'home',\n",
       " 'buying',\n",
       " 'selling',\n",
       " 'My',\n",
       " 'experience',\n",
       " 'Mattison',\n",
       " 'beyond',\n",
       " 'greatly',\n",
       " 'professionally',\n",
       " 'done',\n",
       " 'She',\n",
       " 'really',\n",
       " 'cares',\n",
       " 'clients',\n",
       " 'needs',\n",
       " 'She',\n",
       " 'stays',\n",
       " 'top',\n",
       " 'everything',\n",
       " 'misunderstanding',\n",
       " 'would',\n",
       " 'top',\n",
       " 'Overall',\n",
       " 'truly',\n",
       " 'bends',\n",
       " 'back',\n",
       " 'professionally',\n",
       " 'clients',\n",
       " 'She',\n",
       " 'wonderful',\n",
       " 'person',\n",
       " 'Patrick',\n",
       " 'answered',\n",
       " 'questions',\n",
       " 'email',\n",
       " 'immediately',\n",
       " 'spent',\n",
       " 'lot',\n",
       " 'time',\n",
       " 'making',\n",
       " 'sure',\n",
       " 'I',\n",
       " 'got',\n",
       " 'right',\n",
       " 'loan',\n",
       " 'situation',\n",
       " 'The',\n",
       " 'portal',\n",
       " 'use',\n",
       " 'extremely',\n",
       " 'easy',\n",
       " 'closed',\n",
       " 'refinance',\n",
       " '2',\n",
       " 'weeks',\n",
       " 'Very',\n",
       " 'pleased',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'intend',\n",
       " 'using',\n",
       " 'Patrick',\n",
       " 'refinance',\n",
       " 'later',\n",
       " 'I',\n",
       " 'loved',\n",
       " 'working',\n",
       " 'group',\n",
       " 'people',\n",
       " 'They',\n",
       " 'laughed',\n",
       " 'phone',\n",
       " 'Always',\n",
       " 'answered',\n",
       " 'questions',\n",
       " 'moved',\n",
       " 'loan',\n",
       " 'along',\n",
       " 'quickly',\n",
       " 'possible',\n",
       " 'Knowledgeable',\n",
       " 'staff',\n",
       " 'While',\n",
       " 'shopping',\n",
       " 'around',\n",
       " 'lender',\n",
       " 'one',\n",
       " 'lender',\n",
       " 'badmouthed',\n",
       " 'Wyndham',\n",
       " 'lost',\n",
       " 'business',\n",
       " 'When',\n",
       " 'I',\n",
       " 'told',\n",
       " 'Wyndham',\n",
       " 'class',\n",
       " 'say',\n",
       " 'anything',\n",
       " 'negative',\n",
       " 'another',\n",
       " 'company',\n",
       " 'get',\n",
       " 'sell',\n",
       " 'Great',\n",
       " 'web',\n",
       " 'interface',\n",
       " 'loan',\n",
       " 'application',\n",
       " 'document',\n",
       " 'upload',\n",
       " 'download',\n",
       " 'features',\n",
       " 'Quick',\n",
       " 'responses',\n",
       " 'questions',\n",
       " 'concerns',\n",
       " 'Willingness',\n",
       " 'communicate',\n",
       " 'almost',\n",
       " 'exclusively',\n",
       " 'via',\n",
       " 'email',\n",
       " 'phone',\n",
       " 'calls',\n",
       " 'always',\n",
       " 'option',\n",
       " 'us',\n",
       " 'Working',\n",
       " 'Michelle',\n",
       " 'Wyndham',\n",
       " 'went',\n",
       " 'really',\n",
       " 'well',\n",
       " 'As',\n",
       " 'first',\n",
       " 'time',\n",
       " 'homeowner',\n",
       " 'first',\n",
       " 'refinance',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'process',\n",
       " 'smooth',\n",
       " 'successful',\n",
       " 'I',\n",
       " 'definitely',\n",
       " 'recommend',\n",
       " 'Michelle',\n",
       " 'friends',\n",
       " 'family',\n",
       " 'usten',\n",
       " 'Butler',\n",
       " 'brought',\n",
       " 'humanity',\n",
       " 'connection',\n",
       " 'stiff',\n",
       " 'lifeless',\n",
       " 'scary',\n",
       " 'transaction',\n",
       " 'Not',\n",
       " 'quite',\n",
       " 'possibly',\n",
       " 'largest',\n",
       " 'purchase',\n",
       " 'wife',\n",
       " 'I',\n",
       " 'ever',\n",
       " 'make',\n",
       " 'We',\n",
       " 'simply',\n",
       " 'want',\n",
       " 'transact',\n",
       " 'faceless',\n",
       " 'institution',\n",
       " 'A',\n",
       " 'lot',\n",
       " 'riding',\n",
       " 'picking',\n",
       " 'right',\n",
       " 'house',\n",
       " 'making',\n",
       " 'sure',\n",
       " 'everything',\n",
       " 'would',\n",
       " 'work',\n",
       " 'Austen',\n",
       " 'us',\n",
       " 'every',\n",
       " 'step',\n",
       " 'way',\n",
       " 'It',\n",
       " 'true',\n",
       " 'Austen',\n",
       " 'got',\n",
       " 'us',\n",
       " 'best',\n",
       " 'deal',\n",
       " 'Yes',\n",
       " 'also',\n",
       " 'true',\n",
       " 'But',\n",
       " 'From',\n",
       " 'first',\n",
       " 'starting',\n",
       " 'looking',\n",
       " 'year',\n",
       " 'later',\n",
       " 'found',\n",
       " 'home',\n",
       " 'wanted',\n",
       " 'presence',\n",
       " 'positivity',\n",
       " 'guidance',\n",
       " 'gave',\n",
       " 'us',\n",
       " 'confidence',\n",
       " 'take',\n",
       " 'step',\n",
       " 'move',\n",
       " 'dream',\n",
       " 'home',\n",
       " 'It',\n",
       " 'made',\n",
       " 'HUGE',\n",
       " 'difference',\n",
       " 'family',\n",
       " 'right',\n",
       " 'want',\n",
       " '13',\n",
       " 'kids',\n",
       " 'block',\n",
       " 'happier',\n",
       " 'thanks',\n",
       " 'Austen',\n",
       " 'Butler',\n",
       " 'help',\n",
       " 'Jay',\n",
       " 'easy',\n",
       " 'get',\n",
       " 'hold',\n",
       " 'I',\n",
       " 'questions',\n",
       " 'huge',\n",
       " 'lot',\n",
       " 'whole',\n",
       " 'loan',\n",
       " 'process',\n",
       " 'want',\n",
       " 'know',\n",
       " 'everything',\n",
       " 'right',\n",
       " 'He',\n",
       " 'provided',\n",
       " 'great',\n",
       " 'service',\n",
       " 'I',\n",
       " 'could',\n",
       " 'wanted',\n",
       " 'If',\n",
       " 'I',\n",
       " 'million',\n",
       " 'Friends',\n",
       " 'I',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'Wyndham',\n",
       " 'capital',\n",
       " 'especially',\n",
       " 'Mark',\n",
       " 'Taylor',\n",
       " 'great',\n",
       " 'really',\n",
       " 'help',\n",
       " 'even',\n",
       " 'clock',\n",
       " 'thanks',\n",
       " 'million',\n",
       " 'mark',\n",
       " 'I',\n",
       " 'mean',\n",
       " 'heart',\n",
       " 'I',\n",
       " 'appreciate',\n",
       " 'everything',\n",
       " 'Chaz',\n",
       " 'fantastic',\n",
       " 'throughout',\n",
       " 'entire',\n",
       " 'lending',\n",
       " 'processs',\n",
       " 'I',\n",
       " 'share',\n",
       " 'challenges',\n",
       " 'original',\n",
       " 'underwriter',\n",
       " 'fault',\n",
       " 'Chase',\n",
       " 'yet',\n",
       " 'stepped',\n",
       " 'quickly',\n",
       " 'rectify',\n",
       " 'issues',\n",
       " 'I',\n",
       " 'He',\n",
       " 'glue',\n",
       " 'kept',\n",
       " 'deal',\n",
       " 'together',\n",
       " 'I',\n",
       " 'challenges',\n",
       " 'first',\n",
       " 'property',\n",
       " 'falling',\n",
       " 'last',\n",
       " 'minute',\n",
       " 'yet',\n",
       " 'Chaz',\n",
       " 'kept',\n",
       " 'spirits',\n",
       " 'assisted',\n",
       " 'quickly',\n",
       " 'transitioning',\n",
       " 'new',\n",
       " 'much',\n",
       " 'better',\n",
       " 'property',\n",
       " 'end',\n",
       " 'Again',\n",
       " 'cannot',\n",
       " 'say',\n",
       " 'enough',\n",
       " 'positive',\n",
       " 'things',\n",
       " 'Chaz',\n",
       " 'Consummate',\n",
       " 'professional',\n",
       " 'responsive',\n",
       " 'I',\n",
       " 'would',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'business',\n",
       " 'plan',\n",
       " 'staying',\n",
       " 'touch',\n",
       " 'future',\n",
       " 'opportunities',\n",
       " 'Austen',\n",
       " 'awsome',\n",
       " 'every',\n",
       " 'step',\n",
       " 'way',\n",
       " 'refi',\n",
       " 'new',\n",
       " 'home',\n",
       " 'purchase',\n",
       " 'He',\n",
       " 'explains',\n",
       " 'best',\n",
       " 'options',\n",
       " 'thoroughly',\n",
       " 'B',\n",
       " 'S',\n",
       " 'companies',\n",
       " 'use',\n",
       " 'get',\n",
       " 'business',\n",
       " 'Being',\n",
       " 'repeat',\n",
       " 'customer',\n",
       " 'says',\n",
       " 'The',\n",
       " 'salesperson',\n",
       " 'kept',\n",
       " 'pushing',\n",
       " 'cash',\n",
       " 'refi',\n",
       " 'even',\n",
       " 'told',\n",
       " 'I',\n",
       " 'looking',\n",
       " 'heloc',\n",
       " 'specifically',\n",
       " 'Then',\n",
       " 'hangs',\n",
       " 'I',\n",
       " 'said',\n",
       " 'I',\n",
       " 'quoted',\n",
       " 'better',\n",
       " 'rate',\n",
       " 'TD',\n",
       " 'bank',\n",
       " 'Ridiculous',\n",
       " 'This',\n",
       " 'worst',\n",
       " 'experience',\n",
       " 'ever',\n",
       " 'It',\n",
       " 'like',\n",
       " 'never',\n",
       " 'gone',\n",
       " 'process',\n",
       " 'I',\n",
       " 'could',\n",
       " 'get',\n",
       " 'complete',\n",
       " 'list',\n",
       " 'documents',\n",
       " 'required',\n",
       " 'Every',\n",
       " 'day',\n",
       " 'new',\n",
       " 'request',\n",
       " 'The',\n",
       " 'appraiser',\n",
       " 'hired',\n",
       " 'made',\n",
       " 'many',\n",
       " 'mistakes',\n",
       " 'I',\n",
       " 'send',\n",
       " 'corrections',\n",
       " 'underwriting',\n",
       " 'process',\n",
       " 'go',\n",
       " 'back',\n",
       " 'make',\n",
       " 'corrections',\n",
       " 'Wyndum',\n",
       " 'would',\n",
       " 'continue',\n",
       " 'ask',\n",
       " 'copies',\n",
       " 'documents',\n",
       " 'There',\n",
       " 'many',\n",
       " 'documents',\n",
       " 'I',\n",
       " 'send',\n",
       " '3',\n",
       " '4',\n",
       " 'times',\n",
       " 'lost',\n",
       " 'filed',\n",
       " 'correctly',\n",
       " 'went',\n",
       " '4',\n",
       " 'different',\n",
       " 'people',\n",
       " 'process',\n",
       " 'I',\n",
       " 'would',\n",
       " 'NEVER',\n",
       " 'use',\n",
       " 'Awful',\n",
       " 'experience',\n",
       " 'A',\n",
       " 'good',\n",
       " 'rate',\n",
       " 'frustrating',\n",
       " 'process',\n",
       " 'constant',\n",
       " 'delays',\n",
       " 'decisions',\n",
       " 'made',\n",
       " 'changed',\n",
       " 'overwhelmed',\n",
       " 'underwriting',\n",
       " 'team',\n",
       " '3',\n",
       " 'week',\n",
       " 'back',\n",
       " 'log',\n",
       " 'rookie',\n",
       " 'mistake',\n",
       " 'related',\n",
       " 'Texas',\n",
       " 'law',\n",
       " 'caused',\n",
       " 'additional',\n",
       " '2',\n",
       " 'week',\n",
       " 'wait',\n",
       " 'Closing',\n",
       " 'happened',\n",
       " 'almost',\n",
       " '6',\n",
       " 'weeks',\n",
       " 'Courteous',\n",
       " 'professional',\n",
       " 'knowledgeable',\n",
       " 'sense',\n",
       " 'urgency',\n",
       " 'Would',\n",
       " 'recommend',\n",
       " 'They',\n",
       " 'upfront',\n",
       " 'Learn',\n",
       " 'mistake',\n",
       " 'I',\n",
       " 'trusted',\n",
       " 'loan',\n",
       " 'officer',\n",
       " 'zero',\n",
       " 'point',\n",
       " 'rate',\n",
       " 'option',\n",
       " 'closing',\n",
       " 'loan',\n",
       " 'convinced',\n",
       " 'initially',\n",
       " 'sign',\n",
       " 'Good',\n",
       " 'Faith',\n",
       " 'Estimate',\n",
       " 'higher',\n",
       " 'rate',\n",
       " 'plus',\n",
       " 'lender',\n",
       " 'credit',\n",
       " 'After',\n",
       " 'running',\n",
       " 'numbers',\n",
       " 'lower',\n",
       " 'interest',\n",
       " 'favorable',\n",
       " 'asking',\n",
       " 'I',\n",
       " 'charge',\n",
       " 'points',\n",
       " 'It',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'good',\n",
       " 'words',\n",
       " 'beginning',\n",
       " 'please',\n",
       " 'careful',\n",
       " 'dealing',\n",
       " 'lender',\n",
       " 'Initially',\n",
       " 'Mortgage',\n",
       " 'Broker',\n",
       " 'friendly',\n",
       " 'assertive',\n",
       " 'Once',\n",
       " 'paper',\n",
       " 'work',\n",
       " 'filled',\n",
       " 'closing',\n",
       " 'loan',\n",
       " 'eminent',\n",
       " 'became',\n",
       " 'increasingly',\n",
       " 'difficult',\n",
       " 'reach',\n",
       " 'broker',\n",
       " 'I',\n",
       " 'initiate',\n",
       " 'phone',\n",
       " 'calls',\n",
       " 'It',\n",
       " 'seemed',\n",
       " 'I',\n",
       " 'call',\n",
       " 'make',\n",
       " 'process',\n",
       " 'move',\n",
       " 'forward',\n",
       " 'It',\n",
       " 'took',\n",
       " 'four',\n",
       " 'weeks',\n",
       " 'get',\n",
       " 'copy',\n",
       " 'closing',\n",
       " 'documents',\n",
       " 'payoff',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'Initially',\n",
       " 'Mortgage',\n",
       " 'Broker',\n",
       " 'friendly',\n",
       " 'assertive',\n",
       " 'Once',\n",
       " 'paper',\n",
       " 'work',\n",
       " 'filled',\n",
       " 'closing',\n",
       " 'loan',\n",
       " 'eminent',\n",
       " 'became',\n",
       " 'increasingly',\n",
       " 'difficult',\n",
       " 'reach',\n",
       " 'broker',\n",
       " 'I',\n",
       " 'initiate',\n",
       " 'phone',\n",
       " 'calls',\n",
       " 'It',\n",
       " 'seemed',\n",
       " 'I',\n",
       " 'call',\n",
       " 'make',\n",
       " 'process',\n",
       " 'move',\n",
       " 'forward',\n",
       " 'It',\n",
       " 'took',\n",
       " 'four',\n",
       " 'weeks',\n",
       " 'get',\n",
       " 'copy',\n",
       " 'closing',\n",
       " 'documents',\n",
       " 'payoff',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'I',\n",
       " 'worked',\n",
       " 'Kory',\n",
       " 'Carla',\n",
       " 'NASB',\n",
       " 'They',\n",
       " 'superb',\n",
       " 'team',\n",
       " 'provided',\n",
       " 'excellent',\n",
       " 'responsive',\n",
       " 'knowledgeable',\n",
       " 'service',\n",
       " 'throughout',\n",
       " 'entire',\n",
       " 'process',\n",
       " 'Ours',\n",
       " 'complicated',\n",
       " 'process',\n",
       " 'refinance',\n",
       " 'loan',\n",
       " 'hung',\n",
       " 'us',\n",
       " 'entire',\n",
       " 'time',\n",
       " 'It',\n",
       " 'great',\n",
       " 'experience',\n",
       " 'working',\n",
       " 'I',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'use',\n",
       " 'future',\n",
       " 'mortgage',\n",
       " 'needs',\n",
       " 'Kory',\n",
       " 'far',\n",
       " 'best',\n",
       " 'loan',\n",
       " 'officer',\n",
       " 'I',\n",
       " 'ever',\n",
       " 'worked',\n",
       " 'We',\n",
       " 'dealing',\n",
       " 'complicated',\n",
       " 'refinance',\n",
       " 'purchase',\n",
       " 'two',\n",
       " 'different',\n",
       " 'properties',\n",
       " 'cool',\n",
       " 'calm',\n",
       " 'professional',\n",
       " 'extremely',\n",
       " 'knowledgeable',\n",
       " 'throughout',\n",
       " 'entire',\n",
       " 'process',\n",
       " 'I',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'moving',\n",
       " 'forward',\n",
       " 'His',\n",
       " 'level',\n",
       " 'integrity',\n",
       " 'responsiveness',\n",
       " 'second',\n",
       " 'none',\n",
       " 'Thanks',\n",
       " 'everything',\n",
       " 'Kory',\n",
       " 'Expert',\n",
       " 'loan',\n",
       " 'officer',\n",
       " 'well',\n",
       " 'versed',\n",
       " 'VA',\n",
       " 'financial',\n",
       " 'impacts',\n",
       " 'loan',\n",
       " 'component',\n",
       " 'changes',\n",
       " 'The',\n",
       " 'best',\n",
       " 'I',\n",
       " 'worked',\n",
       " 'across',\n",
       " '7',\n",
       " 'home',\n",
       " 'buying',\n",
       " 'selling',\n",
       " 'deals',\n",
       " 'Now',\n",
       " 'advocated',\n",
       " '1',\n",
       " 'real',\n",
       " 'estate',\n",
       " 'broker',\n",
       " 'state',\n",
       " 'due',\n",
       " 'ability',\n",
       " 'close',\n",
       " 'complicated',\n",
       " 'high',\n",
       " 'pressure',\n",
       " 'deals',\n",
       " 'She',\n",
       " 'places',\n",
       " 'top',\n",
       " 'list',\n",
       " '32',\n",
       " 'years',\n",
       " 'business',\n",
       " 'Professionally',\n",
       " 'patiently',\n",
       " 'answers',\n",
       " 'detailed',\n",
       " 'questions',\n",
       " 'rally',\n",
       " 'expanded',\n",
       " 'NASB',\n",
       " 'team',\n",
       " 'also',\n",
       " 'stellar',\n",
       " 'support',\n",
       " 'needs',\n",
       " 'He',\n",
       " 'handled',\n",
       " 'complicated',\n",
       " 'VA',\n",
       " 'loan',\n",
       " 'seemed',\n",
       " 'change',\n",
       " 'every',\n",
       " 'week',\n",
       " 'tried',\n",
       " 'buy',\n",
       " 'Antebellum',\n",
       " 'home',\n",
       " 'tough',\n",
       " 'comps',\n",
       " 'retirement',\n",
       " 'military',\n",
       " 'tough',\n",
       " 'income',\n",
       " 'verification',\n",
       " 'five',\n",
       " 'families',\n",
       " 'swapping',\n",
       " 'homes',\n",
       " 'tons',\n",
       " 'contingencies',\n",
       " 'Everything',\n",
       " 'along',\n",
       " 'way',\n",
       " 'mattered',\n",
       " 'Aaron',\n",
       " 'NASB',\n",
       " 'team',\n",
       " 'got',\n",
       " 'done',\n",
       " 'even',\n",
       " 'weekends',\n",
       " 'closed',\n",
       " 'time',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords \n",
    "stop = set(stopwords.words(\"english\"))\n",
    "all_words = [w for w in all_words if w not in stop]\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the words\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lmtzr = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "all_words_lmtzr = []\n",
    "\n",
    "for word in all_words:\n",
    "    all_words_lmtzr.append(lmtzr.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22071"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words_lmtzr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequency count for all words\n",
    "from collections import Counter\n",
    "df_freq = pd.DataFrame({\"Words\": list(Counter(all_words).keys()), \"Counts\": list(Counter(all_words).values())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wyndham</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Each</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Words  Counts\n",
       "0    Great      31\n",
       "1      job      25\n",
       "2  Wyndham      16\n",
       "3  Capital      11\n",
       "4     Each       2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Key Positive/Negative words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment.vader as senti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_senti=[]\n",
    "for i in df_freq.Words:\n",
    "    list_senti.append(sid.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_senti=pd.DataFrame(list_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_senti\n",
    "final_senti=pd.concat((df_freq,list_senti),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Counts</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wyndham</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capital</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Each</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>grandson</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>snail</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>kindest</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>empathetic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2900 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Words  Counts  neg  neu  pos  compound\n",
       "0          Great      31  0.0  0.0  1.0    0.6249\n",
       "1            job      25  0.0  1.0  0.0    0.0000\n",
       "2        Wyndham      16  0.0  1.0  0.0    0.0000\n",
       "3        Capital      11  0.0  1.0  0.0    0.0000\n",
       "4           Each       2  0.0  1.0  0.0    0.0000\n",
       "...          ...     ...  ...  ...  ...       ...\n",
       "2895    grandson       1  0.0  1.0  0.0    0.0000\n",
       "2896       snail       1  0.0  1.0  0.0    0.0000\n",
       "2897     kindest       1  0.0  1.0  0.0    0.0000\n",
       "2898  empathetic       1  0.0  0.0  1.0    0.4019\n",
       "2899          58       1  0.0  1.0  0.0    0.0000\n",
       "\n",
       "[2900 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_senti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequent positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Counts</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>recommend</td>\n",
       "      <td>152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>great</td>\n",
       "      <td>108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>best</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>easy</td>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>helpful</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>illiterate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>grandson</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>snail</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>kindest</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2900 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Words  Counts  neg  neu  pos  compound\n",
       "54     recommend     152  0.0  0.0  1.0    0.3612\n",
       "273        great     108  0.0  0.0  1.0    0.6249\n",
       "245         best      88  0.0  0.0  1.0    0.6369\n",
       "148         easy      63  0.0  0.0  1.0    0.4404\n",
       "16       helpful      56  0.0  0.0  1.0    0.4215\n",
       "...          ...     ...  ...  ...  ...       ...\n",
       "2894  illiterate       1  0.0  1.0  0.0    0.0000\n",
       "2895    grandson       1  0.0  1.0  0.0    0.0000\n",
       "2896       snail       1  0.0  1.0  0.0    0.0000\n",
       "2897     kindest       1  0.0  1.0  0.0    0.0000\n",
       "2899          58       1  0.0  1.0  0.0    0.0000\n",
       "\n",
       "[2900 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_senti.sort_values(by=['pos','Counts'],ascending=[False,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequent negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Counts</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>hard</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>pay</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>stressful</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>lower</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>problem</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>grandson</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>snail</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>kindest</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>empathetic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2900 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Words  Counts  neg  neu  pos  compound\n",
       "639         hard      42  1.0  0.0  0.0   -0.1027\n",
       "1220         pay      25  1.0  0.0  0.0   -0.1027\n",
       "65     stressful      24  1.0  0.0  0.0   -0.5106\n",
       "423        lower      23  1.0  0.0  0.0   -0.2960\n",
       "750      problem      18  1.0  0.0  0.0   -0.4019\n",
       "...          ...     ...  ...  ...  ...       ...\n",
       "2895    grandson       1  0.0  1.0  0.0    0.0000\n",
       "2896       snail       1  0.0  1.0  0.0    0.0000\n",
       "2897     kindest       1  0.0  1.0  0.0    0.0000\n",
       "2898  empathetic       1  0.0  0.0  1.0    0.4019\n",
       "2899          58       1  0.0  1.0  0.0    0.0000\n",
       "\n",
       "[2900 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_senti.sort_values(by=['neg','Counts'],ascending=[False,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Clssification of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Great job, Wyndham Capital! Each person was pr...\n",
       "1      Matthew Richardson is professional and helpful...\n",
       "2      We had a past experience with Wyndham Mortgage...\n",
       "3      We have been dealing with Brad Thomka from the...\n",
       "4      I can't express how grateful I am for the supp...\n",
       "                             ...                        \n",
       "500    \\r\\nI never write reviews but had to this time...\n",
       "501    \\r\\nIt all started when Bob G ran a credit che...\n",
       "502    \\r\\nWhat a horrible experience. We have excell...\n",
       "503    \\r\\nRep was extremely professional, friendly, ...\n",
       "504    \\r\\nI was working with a loan consultant from ...\n",
       "Name: Reviews, Length: 505, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\").tokenize\n",
    "all_words = []\n",
    "for review in corpus:\n",
    "    sentence = []\n",
    "    sentence.append(tokenizer(review))\n",
    "    all_words.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prash\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    words=nltk.word_tokenize(corpus[i])\n",
    "    words=[wc.lemmatize(word) for word in words]\n",
    "    corpus[i]=' '.join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Great job , Wyndham Capital ! Each person wa p...\n",
       "1      Matthew Richardson is professional and helpful...\n",
       "2      We had a past experience with Wyndham Mortgage...\n",
       "3      We have been dealing with Brad Thomka from the...\n",
       "4      I ca n't express how grateful I am for the sup...\n",
       "                             ...                        \n",
       "500    I never write review but had to this time to p...\n",
       "501    It all started when Bob G ran a credit check w...\n",
       "502    What a horrible experience . We have excellent...\n",
       "503    Rep wa extremely professional , friendly , and...\n",
       "504    I wa working with a loan consultant from NASB ...\n",
       "Name: Reviews, Length: 505, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_senti=[]\n",
    "for i in corpus:\n",
    "    corpus_senti.append(sid.polarity_scores(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.8011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.8516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.9595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.9818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.5569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.086</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.9350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.4065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.161</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.7970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.9805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.017</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.9537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       neg    neu    pos  compound\n",
       "0    0.000  0.690  0.310    0.8011\n",
       "1    0.000  0.693  0.307    0.8516\n",
       "2    0.000  0.789  0.211    0.9595\n",
       "3    0.019  0.750  0.231    0.9818\n",
       "4    0.040  0.863  0.097    0.5569\n",
       "..     ...    ...    ...       ...\n",
       "500  0.086  0.801  0.113    0.9350\n",
       "501  0.036  0.905  0.059    0.4065\n",
       "502  0.161  0.722  0.116   -0.7970\n",
       "503  0.012  0.846  0.142    0.9805\n",
       "504  0.017  0.878  0.106    0.9537\n",
       "\n",
       "[505 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpus_senti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Identifying key themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Topic modelling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [great, wyndham, capit, person, profession, he...\n",
       "1    [matthew, richardson, profession, help, help, ...\n",
       "2    [past, experi, wyndham, mortgag, question, nee...\n",
       "3    [deal, brad, thomka, begin, start, stress, tim...\n",
       "4    [express, grate, support, zach, provid, famili...\n",
       "5    [pleasur, work, wyndham, capit, septemb, famil...\n",
       "6    [experi, mattison, great, profession, care, cl...\n",
       "7    [patrick, answer, question, email, immedi, spe...\n",
       "8    [love, work, group, peopl, laugh, phone, answe...\n",
       "9    [great, interfac, loan, applic, document, uplo...\n",
       "Name: Reviews, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = data['Reviews'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Lisa', 'Podorson', 'ha', 'set', 'my', 'closing', 'date', 'three', 'different', 'date', ',', 'which', 'she', 'ha', 'reneged', 'on', 'since', 'August', '2017', '.', 'Ms.', 'Podorson', 'ha', 'continued', 'to', 'asked', 'for', 'document', 'at', 'the', 'last', 'minuet', '.', 'She', 'doe', 'not', 'return', 'text', 'or', 'phone', 'call', 'for', 'an', 'entire', 'week', '.', 'She', 'did', 'not', 'return', 'my', 'call', 'until', 'I', 'decided', 'to', 'cancel', 'the', 'refinancing', 'and', 'Contact', 'the', 'BBB', '.', 'She', 'ha', 'been', 'very', 'unprofessional', 'during', 'this', 'process', '.', 'This', 'experience', 'wa', 'very', 'unpleasant', '...', 'Read', 'More']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['lisa', 'podorson', 'close', 'date', 'differ', 'date', 'reneg', 'august', 'podorson', 'continu', 'ask', 'document', 'minuet', 'return', 'text', 'phone', 'entir', 'week', 'return', 'decid', 'cancel', 'refin', 'contact', 'unprofession', 'process', 'experi', 'unpleas', 'read']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = data.Reviews[100]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary from â€˜processed_docsâ€™ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 capit\n",
      "1 great\n",
      "2 help\n",
      "3 person\n",
      "4 process\n",
      "5 profession\n",
      "6 refin\n",
      "7 smooth\n",
      "8 thank\n",
      "9 wyndham\n",
      "10 correct\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out tokens that appear in\n",
    "## *less than 15 documents (absolute number) or\n",
    "## *more than 0.5 documents (fraction of total corpus size, not absolute number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each document we create a dictionary reporting how many words and how many times those words appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (13, 1),\n",
       " (16, 1),\n",
       " (22, 1),\n",
       " (47, 1),\n",
       " (56, 1),\n",
       " (67, 1),\n",
       " (78, 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 3 (\"process\") appears 1 time.\n",
      "Word 5 (\"refin\") appears 1 time.\n",
      "Word 12 (\"experi\") appears 1 time.\n",
      "Word 21 (\"entir\") appears 1 time.\n",
      "Word 35 (\"close\") appears 1 time.\n",
      "Word 46 (\"week\") appears 1 time.\n",
      "Word 51 (\"phone\") appears 1 time.\n",
      "Word 55 (\"document\") appears 1 time.\n",
      "Word 60 (\"differ\") appears 1 time.\n",
      "Word 79 (\"ask\") appears 1 time.\n",
      "Word 95 (\"contact\") appears 1 time.\n",
      "Word 98 (\"read\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#Preview\n",
    "bow_doc_100 = bow_corpus[100]\n",
    "for i in range(len(bow_doc_100)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_100[i][0], \n",
    "                                               dictionary[bow_doc_100[i][0]], \n",
    "bow_doc_100[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tf-idf model object using models.TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.2799108301911471),\n",
      " (1, 0.2766749960867732),\n",
      " (2, 0.5031979284661511),\n",
      " (3, 0.18026193319570374),\n",
      " (4, 0.34306630631061447),\n",
      " (5, 0.3476505132900647),\n",
      " (6, 0.4834341708480756),\n",
      " (7, 0.2951853425392064)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.045*\"work\" + 0.041*\"loan\" + 0.037*\"bank\" + 0.034*\"compani\" + 0.032*\"process\" + 0.030*\"time\" + 0.030*\"close\" + 0.029*\"friend\" + 0.025*\"help\" + 0.025*\"go\"\n",
      "Topic: 1 \n",
      "Words: 0.053*\"work\" + 0.049*\"loan\" + 0.045*\"process\" + 0.038*\"team\" + 0.035*\"profession\" + 0.033*\"time\" + 0.031*\"mortgag\" + 0.028*\"respons\" + 0.027*\"question\" + 0.026*\"great\"\n",
      "Topic: 2 \n",
      "Words: 0.055*\"recommend\" + 0.052*\"rate\" + 0.045*\"best\" + 0.044*\"mortgag\" + 0.041*\"experi\" + 0.040*\"process\" + 0.035*\"servic\" + 0.030*\"home\" + 0.029*\"compani\" + 0.027*\"offer\"\n",
      "Topic: 3 \n",
      "Words: 0.064*\"close\" + 0.059*\"time\" + 0.052*\"great\" + 0.044*\"home\" + 0.040*\"servic\" + 0.036*\"process\" + 0.032*\"team\" + 0.028*\"loan\" + 0.028*\"read\" + 0.027*\"nasb\"\n",
      "Topic: 4 \n",
      "Words: 0.051*\"help\" + 0.051*\"close\" + 0.039*\"process\" + 0.035*\"email\" + 0.032*\"loan\" + 0.029*\"week\" + 0.029*\"recommend\" + 0.028*\"read\" + 0.027*\"time\" + 0.024*\"work\"\n",
      "Topic: 5 \n",
      "Words: 0.050*\"close\" + 0.048*\"loan\" + 0.036*\"send\" + 0.036*\"lender\" + 0.030*\"rate\" + 0.029*\"servic\" + 0.027*\"say\" + 0.026*\"tell\" + 0.025*\"read\" + 0.025*\"nasb\"\n",
      "Topic: 6 \n",
      "Words: 0.057*\"work\" + 0.047*\"experi\" + 0.046*\"time\" + 0.043*\"home\" + 0.036*\"rate\" + 0.035*\"best\" + 0.034*\"refin\" + 0.032*\"thank\" + 0.032*\"nasb\" + 0.030*\"process\"\n",
      "Topic: 7 \n",
      "Words: 0.095*\"work\" + 0.045*\"process\" + 0.037*\"home\" + 0.036*\"read\" + 0.030*\"recommend\" + 0.030*\"begin\" + 0.026*\"great\" + 0.025*\"pleasur\" + 0.025*\"help\" + 0.025*\"abl\"\n",
      "Topic: 8 \n",
      "Words: 0.048*\"hous\" + 0.046*\"question\" + 0.042*\"communic\" + 0.040*\"work\" + 0.038*\"thank\" + 0.033*\"recommend\" + 0.031*\"process\" + 0.024*\"buy\" + 0.024*\"home\" + 0.022*\"time\"\n",
      "Topic: 9 \n",
      "Words: 0.098*\"loan\" + 0.052*\"work\" + 0.046*\"time\" + 0.039*\"close\" + 0.034*\"process\" + 0.028*\"go\" + 0.027*\"read\" + 0.025*\"help\" + 0.024*\"great\" + 0.021*\"need\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.044*\"help\" + 0.030*\"friend\" + 0.028*\"thank\" + 0.027*\"possibl\" + 0.027*\"abl\" + 0.026*\"lender\" + 0.025*\"best\" + 0.025*\"great\" + 0.025*\"offer\" + 0.023*\"home\"\n",
      "Topic: 1 Word: 0.036*\"respons\" + 0.029*\"close\" + 0.027*\"team\" + 0.025*\"time\" + 0.024*\"smooth\" + 0.024*\"question\" + 0.024*\"lender\" + 0.023*\"definit\" + 0.023*\"profession\" + 0.021*\"work\"\n",
      "Topic: 2 Word: 0.053*\"best\" + 0.053*\"good\" + 0.043*\"deal\" + 0.040*\"hard\" + 0.036*\"work\" + 0.033*\"offic\" + 0.031*\"respons\" + 0.028*\"loan\" + 0.020*\"lender\" + 0.020*\"receiv\"\n",
      "Topic: 3 Word: 0.025*\"work\" + 0.025*\"loan\" + 0.024*\"answer\" + 0.022*\"high\" + 0.022*\"contact\" + 0.022*\"inform\" + 0.021*\"process\" + 0.020*\"email\" + 0.020*\"patient\" + 0.020*\"hous\"\n",
      "Topic: 4 Word: 0.029*\"pleasur\" + 0.029*\"communic\" + 0.027*\"profession\" + 0.027*\"document\" + 0.024*\"team\" + 0.022*\"excel\" + 0.022*\"question\" + 0.022*\"refin\" + 0.021*\"loan\" + 0.020*\"keep\"\n",
      "Topic: 5 Word: 0.042*\"person\" + 0.036*\"loan\" + 0.034*\"team\" + 0.032*\"refin\" + 0.029*\"nasb\" + 0.028*\"profession\" + 0.028*\"close\" + 0.027*\"offic\" + 0.026*\"custom\" + 0.026*\"home\"\n",
      "Topic: 6 Word: 0.040*\"want\" + 0.029*\"home\" + 0.026*\"famili\" + 0.024*\"extrem\" + 0.024*\"best\" + 0.023*\"nasb\" + 0.023*\"respons\" + 0.023*\"process\" + 0.022*\"go\" + 0.020*\"purchas\"\n",
      "Topic: 7 Word: 0.046*\"servic\" + 0.030*\"great\" + 0.029*\"mortgag\" + 0.026*\"provid\" + 0.025*\"custom\" + 0.021*\"process\" + 0.021*\"help\" + 0.021*\"close\" + 0.021*\"team\" + 0.020*\"knowledg\"\n",
      "Topic: 8 Word: 0.028*\"close\" + 0.027*\"know\" + 0.026*\"help\" + 0.025*\"rate\" + 0.025*\"time\" + 0.025*\"send\" + 0.024*\"expect\" + 0.024*\"abl\" + 0.023*\"take\" + 0.023*\"good\"\n",
      "Topic: 9 Word: 0.040*\"mortgag\" + 0.035*\"great\" + 0.031*\"home\" + 0.031*\"purchas\" + 0.030*\"extrem\" + 0.027*\"help\" + 0.026*\"process\" + 0.024*\"experi\" + 0.023*\"knowledg\" + 0.021*\"know\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fit(data.Reviews.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=tf.transform(data.Reviews.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls=KMeans(n_clusters=4,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=1234, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Predicting Star ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y=train_test_split(data.Reviews,data.Stars,test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fit(data.Reviews.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X_Tfidf = tf.transform(train_x)\n",
    "Test_X_Tfidf = tf.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Naive.fit(Train_X_Tfidf,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y=Naive.predict(Test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8157894736842105"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(pred_y,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 5, 1, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 1, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 1, 1, 5, 5, 5, 5, 5,\n",
       "       5, 5, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 5, 1, 5, 1, 5, 5, 1, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 5, 5, 5, 5, 5, 5],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM.predict(Test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(SVM.predict(Test_X_Tfidf),test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Intent Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Intent Analysis and Topic modelling are same and topic modelling is done already in part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.082*\"loan\" + 0.054*\"home\" + 0.052*\"great\" + 0.050*\"close\" + 0.050*\"nasb\" + 0.044*\"know\" + 0.039*\"team\" + 0.031*\"help\" + 0.031*\"time\" + 0.030*\"read\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.053*\"question\" + 0.046*\"process\" + 0.045*\"recommend\" + 0.036*\"answer\" + 0.034*\"help\" + 0.032*\"work\" + 0.032*\"rate\" + 0.030*\"home\" + 0.025*\"time\" + 0.024*\"experi\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.064*\"email\" + 0.040*\"thank\" + 0.037*\"lender\" + 0.036*\"receiv\" + 0.032*\"respons\" + 0.032*\"rate\" + 0.026*\"say\" + 0.026*\"home\" + 0.026*\"phone\" + 0.025*\"custom\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.087*\"work\" + 0.053*\"process\" + 0.029*\"refin\" + 0.028*\"great\" + 0.028*\"time\" + 0.028*\"read\" + 0.026*\"loan\" + 0.025*\"close\" + 0.023*\"recommend\" + 0.022*\"respons\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.053*\"close\" + 0.052*\"process\" + 0.051*\"loan\" + 0.038*\"work\" + 0.037*\"servic\" + 0.037*\"recommend\" + 0.030*\"week\" + 0.029*\"home\" + 0.028*\"help\" + 0.027*\"experi\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.048*\"bank\" + 0.040*\"work\" + 0.040*\"time\" + 0.035*\"go\" + 0.034*\"close\" + 0.034*\"document\" + 0.034*\"experi\" + 0.033*\"week\" + 0.031*\"home\" + 0.030*\"process\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.047*\"time\" + 0.043*\"lender\" + 0.042*\"send\" + 0.040*\"rate\" + 0.034*\"read\" + 0.034*\"respons\" + 0.028*\"week\" + 0.025*\"take\" + 0.025*\"call\" + 0.022*\"loan\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.048*\"mortgag\" + 0.048*\"compani\" + 0.037*\"home\" + 0.037*\"loan\" + 0.031*\"recommend\" + 0.029*\"help\" + 0.028*\"close\" + 0.024*\"busi\" + 0.021*\"deal\" + 0.021*\"best\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.069*\"time\" + 0.059*\"loan\" + 0.045*\"process\" + 0.030*\"read\" + 0.029*\"take\" + 0.029*\"communic\" + 0.028*\"offic\" + 0.023*\"work\" + 0.021*\"rate\" + 0.021*\"feel\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.070*\"loan\" + 0.056*\"close\" + 0.033*\"time\" + 0.031*\"mortgag\" + 0.029*\"read\" + 0.028*\"process\" + 0.027*\"nasb\" + 0.025*\"work\" + 0.024*\"rate\" + 0.024*\"home\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
